{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import pandas as pd\n",
    "import time\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeit():\n",
    "    from datetime import datetime\n",
    "    def __enter__(self):\n",
    "        self.tic = self.datetime.now()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print('runtime: {}'.format(self.datetime.now() - self.tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to collect from Airbnb and TripAdvisor: \n",
    "# url / about / name / price / lat_lon / address (zip code for Airbnb) / review_num / rating / photos (4)  \n",
    "def scrape(items, base_url, key):\n",
    "    with timeit():\n",
    "        browser = init_browser()\n",
    "        data = []\n",
    "        i = 0\n",
    "        for item in items:\n",
    "            if (\"plus\" in item) == False:\n",
    "                i+=1\n",
    "                url = base_url+item\n",
    "                browser.visit(url)\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    details = {}\n",
    "                    photos = []\n",
    "\n",
    "                    soup = BeautifulSoup(browser.html, \"html.parser\")\n",
    "\n",
    "                    # Get name\n",
    "                    name = soup.find(\"h1\", class_=\"ui_header h1\").text\n",
    "\n",
    "                    # Get price\n",
    "                    price = soup.find('div', class_='price_night').div.text.split('$')[1]\n",
    "\n",
    "                    # Get description\n",
    "                    try:\n",
    "                        about = soup.find('div', class_='common-text-ReadMore__content--2X4LR').text\n",
    "                    except:\n",
    "                        about = \"\"\n",
    "\n",
    "                    # Get latitude and logitude\n",
    "                    try:\n",
    "                        lat_lon = soup.find('img', class_='hotels-hotel-review-location-StaticMap__map--3L4sb').attrs['src'].split('center=')[1].split('&')[0].split(',')\n",
    "                    except:\n",
    "                        lat_lon = \"\"\n",
    "                        print(\"LAT_LON does not exist for: \" + url)\n",
    "                        print(\"Index #\" + i)\n",
    "                        \n",
    "                    # Get address\n",
    "                    address = soup.find('span', class_='public-business-listing-ContactInfo__ui_link--1_7Zp public-business-listing-ContactInfo__level_4--3JgmI').text\n",
    "\n",
    "                    # Get review number\n",
    "                    review_num = soup.find('span', class_='hotels-hotel-review-about-with-photos-Reviews__seeAllReviews--3PpLR').text.split(\" \")[0]\n",
    "\n",
    "                    # Get rating\n",
    "                    rating = soup.find('span', class_='hotels-hotel-review-about-with-photos-Reviews__overallRating--vElGA').text  \n",
    "\n",
    "                    # Get photos\n",
    "                    photos_text = soup.find_all(\"div\", class_=\"media-image-ResponsiveImage__default--1s-9x\")\n",
    "                    i=0 \n",
    "                    for photo in photos_text:\n",
    "                        i+=1;\n",
    "                        if (i<5):\n",
    "                            photos.append(photo.get('style').split('url(')[1].split(')')[0])\n",
    "                        else:\n",
    "                            break;\n",
    "\n",
    "                    details = {\n",
    "                        \"listing_url\": url,\n",
    "                        \"listing_name\": name,\n",
    "                        \"listing_price\": price,\n",
    "                        \"listing_about\": about,\n",
    "                        \"listing_lat_lon\":lat_lon,\n",
    "                        \"listing_address\": address,\n",
    "                        \"listing_review_num\": review_num,\n",
    "                        \"listing_rating\": rating,\n",
    "                        \"listing_photos\": photos\n",
    "                    }\n",
    "                except:\n",
    "                    print(\"Error : \" + url)\n",
    "                details.pop('_id', None)\n",
    "                data.append(details)\n",
    "        browser.quit()\n",
    "        print(\"City Processed: \" + key.upper())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = {}\n",
    "\n",
    "browser = init_browser()\n",
    "\n",
    "base_url = \"https://www.tripadvisor.com\"\n",
    "uris = [{\"bangkok\":\"/Hotels-g293916-Bangkok-Hotels.html\"},\n",
    "        {\"london\":\"/Hotels-g186338-London_England-Hotels.html\"},\n",
    "        {\"paris\":\"/Hotels-g187147-Paris_Ile_de_France-Hotels.html\"},\n",
    "        {\"dubai\":\"/Hotels-g295424-Dubai_Emirate_of_Dubai-Hotels.html\"},\n",
    "        {\"singapore\":\"/Hotels-g294265-Singapore-Hotels.html\"},\n",
    "        {\"nyc\":\"/Hotels-g60763-New_York_City_New_York-Hotels.html\"},\n",
    "        {\"kl\":\"/Hotels-g298570-Kuala_Lumpur_Wilayah_Persekutuan-Hotels.html\"},\n",
    "        {\"tokyo\":\"/Hotels-g298184-Tokyo_Tokyo_Prefecture_Kanto-Hotels.html\"},\n",
    "        {\"istanbul\":\"/Hotels-g293974-Istanbul-Hotels.html\"},\n",
    "        {\"seoul\":\"/Hotels-g294197-Seoul-Hotels.html\"}\n",
    "       ]\n",
    "\n",
    "for uri in uris:\n",
    "    for key, value in uri.items():\n",
    "        items = []\n",
    "        try:\n",
    "            browser.visit(base_url+value)\n",
    "            counter = 0\n",
    "            state = True\n",
    "            while state:\n",
    "                time.sleep(5)\n",
    "                soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "                page = soup.find_all('div', class_=\"listing_title\")\n",
    "                for item in page:\n",
    "                    link = item.find('a').get('href')\n",
    "                    items.append(link)        \n",
    "    #                 browser.find_by_text('Next').last.click()\n",
    "                browser.find_by_css('a[class=\"nav next taLnk ui_button primary\"]').click()\n",
    "                counter += 1\n",
    "                if counter == 15:\n",
    "                    state = False\n",
    "        except:\n",
    "            print(\"City: \" + key.upper() + \" - URL gathering completed\")\n",
    "        \n",
    "        listings[key] = scrape(items, base_url, key)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of array for a backup source\n",
    "listings_copy = listings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of keys should be 10\n",
    "len(listings_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the list of keys\n",
    "listings_copy.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove _id from the list of keys\n",
    "listings_copy.pop('_id', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "# connect = 'mongodb://localhost:27017'\n",
    "connect = 'mongodb+srv://yuj:explorer2019@city-explorer-ocvlm.mongodb.net/test?retryWrites=true'\n",
    "client = MongoClient(connect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.city_explorer\n",
    "city_tripadvisor = db.city_tripadvisor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_tripadvisor.insert_one(listings_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging the list of lat_lon. There are values that might be empty\n",
    "# df = pd.DataFrame.from_dict(listings_copy['singapore'])['listing_lat_lon']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays full list of output\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dict to json for backup\n",
    "# _id must be removed before using\n",
    "import json\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(listings_copy, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual approach to clean out empty dictionaries\n",
    "# dubai1 = {\"dubai\": [i for i in dict2['dubai'] if i]}\n",
    "# kl1 = {\"kl\": [i for i in dict2['kl'] if i]}\n",
    "# bangkok1 = {\"bangkok\": [i for i in dict2['bangkok'] if i]}\n",
    "# london1 = {\"london\": [i for i in dict2['london'] if i]}\n",
    "# paris1 = {\"paris\": [i for i in dict2['paris'] if i]}\n",
    "# singapore1 = {\"singapore\": [i for i in dict2['singapore'] if i]}\n",
    "# nyc1 = {\"nyc\": [i for i in dict2['nyc'] if i]}\n",
    "# tokyo1 = {\"tokyo\": [i for i in dict2['tokyo'] if i]}\n",
    "# istanbul1 = {\"istanbul\": [i for i in dict2['istanbul'] if i]}\n",
    "# seoul1 = {\"seoul\": [i for i in dict2['seoul'] if i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_listing = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_listing.update(seoul1) # Must be done with all dictionaries after the cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
